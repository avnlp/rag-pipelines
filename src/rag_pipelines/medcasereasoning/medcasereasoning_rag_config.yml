# MedCaseReasoning RAG Configuration

# Dataset configuration
dataset:
  path: "zou-lab/MedCaseReasoning"
  split: "test"
  question_field: "case_prompt"
  answer_field: "final_diagnosis"

# Metadata schema for clinical conversations
metadata_schema:
  properties:
    medical_domain:
      type: "string"
      enum: ["cardiology", "pulmonology", "pediatrics", "congenital_disorders", "general_medicine"]
      description: "Primary medical specialty relevant to the case"
    patient_age_group:
      type: "string"
      enum: ["neonate", "infant", "child", "adolescent", "adult"]
      description: "Age group of the patient described"
    key_findings:
      type: "string"
      description: "Most important clinical findings from the case"
    symptom_context:
      type: "string"
      enum: ["feeding", "rest", "exercise", "sleep", "variable"]
      description: "When the symptoms primarily occur"
    cardiac_findings_present:
      type: "boolean"
      description: "Whether cardiac abnormalities are mentioned in physical exam"
    congenital_condition_suspected:
      type: "boolean"
      description: "Whether this appears to be a congenital condition"

# LLM configurations
llm:
  extractor:
    model: "llama-3.3-70b-versatile"
    temperature: 0
    max_tokens: 1000
    max_retries: 3
  response:
    model: "qwen/qwen3-32b"
    temperature: 0
    max_tokens: 1000
    max_retries: 3
    reasoning_format: "parsed"

# Embedding model
embedding:
  model_name: "Qwen/Qwen3-Embedding-0.6B"

# Contextual Reranker
reranker:
  model: "ContextualAI/ctxl-rerank-v2-instruct-multilingual-1b"
  instruction: "Prioritize Medical articles that directly answer the question."

# Vector store (Milvus)
vectorstore:
  collection_name: "pubmedqa"
  vector_fields: ["dense", "sparse"]
  consistency_level: "Strong"
  drop_old: false

# Retriever
retriever:
  k: 5

# RAG prompt
prompt:
  system_message: |
    You are an expert biomedical researcher. Use the retrieved context
    to answer the question precisely. Provide a detailed and accurate
    answer based on the provided context.
  human_message: "Context:\n{context}\n\nQuestion: {question}"

# Evaluation metrics thresholds and models
metrics:
  contextual_recall:
    threshold: 0.7
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  contextual_precision:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  contextual_relevancy:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  answer_relevancy:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  faithfulness:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

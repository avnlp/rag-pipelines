# MedCaseReasoning RAG Configuration

# Dataset configuration
dataset:
  path: "zou-lab/MedCaseReasoning"
  split: "test"
  question_field: "case_prompt"
  answer_field: "final_diagnosis"

# Metadata schema for MedCaseReasoning
metadata_schema:
  properties:
    clinical_specialty:
      type: "string"
      description: "Medical specialty or department (e.g., cardiology, neurology, emergency medicine)"
    case_type:
      type: "string"
      description: "Type of clinical scenario (e.g., diagnostic, management, referral, triage)"
    patient_age:
      type: "string"
      description: "Patient age group or specific age if mentioned (e.g., '45-year-old', 'elderly', 'pediatric')"
    patient_gender:
      type: "string"
      description: "Patient gender (e.g., male, female, unspecified)"
    chief_complaint:
      type: "string"
      description: "Primary symptom or reason for encounter"
    key_findings:
      type: "string"
      description: "Critical signs, symptoms, or test results from the case"



# Embedding model
embedding:
  model_name: "Qwen/Qwen3-Embedding-0.6B"

# Contextual Reranker
reranker:
  model: "ContextualAI/ctxl-rerank-v2-instruct-multilingual-1b"
  instruction: "Prioritize Medical articles that directly answer the question."

# Vector store (Milvus)
vectorstore:
  collection_name: "medcasereasoning"
  vector_fields: ["dense", "sparse"]
  consistency_level: "Strong"
  drop_old: false

# Retriever
retriever:
  k: 5

# Evaluation metrics thresholds and models
metrics:
  contextual_recall:
    threshold: 0.7
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  contextual_precision:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  contextual_relevancy:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  answer_relevancy:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  faithfulness:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

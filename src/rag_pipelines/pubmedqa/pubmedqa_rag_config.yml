# PubMedQA RAG Configuration

# Dataset configuration
dataset:
  path: "qiaojin/PubMedQA"
  split_name: "pqa_artificial"
  split: "train[:2]"
  question_field: "question"
  answer_field: "long_answer"

# Chunking configuration
chunking:
    chunking_strategy: "by_title"
    max_characters: 500
    new_after_n_chars: 500
    overlap: 100
    overlap_all: false
    combine_text_under_n_chars: 500
    include_orig_elements: true
    multipage_sections: true

# Metadata schema for PubMedQA
metadata_schema:
  properties:
    diseases_conditions:
      type: "string"
      description: "Diseases, disorders, or medical conditions mentioned in the text"
    biological_entities:
      type: "string"
      description: "Genes, proteins, cells, molecules, or biological pathways studied"
    species:
      type: "string"
      description: "Species involved in the study (e.g., human, mouse, rat)"
    study_type:
      type: "string"
      description: "Type of research study design"
    main_findings:
      type: "string"
      description: "Key results or conclusions from the study"
    measurements_assays:
      type: "string"
      description: "Experimental methods, assays, or measurements used"
    tissues_organs:
      type: "string"
      description: "Tissues, organs, or biological systems studied"
    interventions_treatments:
      type: "string"
      description: "Treatments, interventions, or manipulations performed"
    population_cohort:
      type: "string"
      description: "Description of study population or cohort characteristics"
    statistical_significance:
      type: "string"
      description: "Overall statistical significance of main findings"
    effect_direction:
      type: "string"
      description: "Direction of main effects reported"



# Embedding model
embedding:
  model_name: "Qwen/Qwen3-Embedding-0.6B"

# Contextual Reranker and Hybrid Search Weights
reranker:
  model: "ContextualAI/ctxl-rerank-v2-instruct-multilingual-1b"
  instruction: "Prioritize Medical articles that directly answer the question."
  # Weights for hybrid search: [dense_weight, sparse_weight]
  # Dense (semantic) weight: 0.6, Sparse (keyword) weight: 0.4
  hybrid_weights: [0.6, 0.4]

# Vector store (Milvus)
vectorstore:
  collection_name: "pubmedqa"
  vector_fields: ["dense", "sparse"]
  consistency_level: "Strong"
  drop_old: false

# Retriever
retriever:
  k: 5
  rrf_k: 60

# Evaluation metrics thresholds and models
metrics:
  contextual_recall:
    threshold: 0.7
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  contextual_precision:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  contextual_relevancy:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  answer_relevancy:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

  faithfulness:
    threshold: 0.5
    model: "llama-3.3-70b-versatile"
    include_reason: true
    async_mode: true

# MetadataEnricher configuration
metadata_enricher:
  # Enrichment mode: "full" (all layers), "dynamic" (structural + user schema), "minimal" (structural only)
  # Use "dynamic" for query-time extraction, "full" for document indexing
  mode: "full"
  # Batch size for concurrent document enrichment
  batch_size: 10
  # LRU cache size for enriched results deduplication
  cache_size: 1000
